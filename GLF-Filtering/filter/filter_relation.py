# -*- coding: utf-8 -*-
"""filter_relation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n58rfp9Z-rq9b2wu_LOm74FaKE2WwORJ
"""
import os
import pandas as pd
from pandas.errors import EmptyDataError
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def list_files(directory):
    files = []
    for filename in os.listdir(directory):
        path = os.path.join(directory, filename)
        if os.path.isfile(path):
            files.append(filename)
    return files

DATASETS = ["dblp", "dblp", "hprd", "human", "human", "wordnet", "yeast", "yeast"]

to_group = 'vertex_count'
to_drop = ['vertex_id', 'vertex_count', 'edge_count', 'diameter', 'density']
to_drop.remove(to_group)

full_gf_df = pd.read_csv("./GF.csv")

orbits = ['o$_{'+str(i)+"}$" for i in range(15)]
to_exclude = ['o$_{0}$', 'o$_{2}$', 'o$_{7}$']
reduced_orbits = [x for x in orbits if x not in to_exclude]
cross_product = [(x, y) for x in full_gf_df[to_group].unique() for y in reduced_orbits]

fig = plt.figure(figsize=(10, 10))
fig.subplots_adjust(hspace=0.4, wspace=0.25)
i = 1

for DATASET in DATASETS:
  print("Started:", DATASET)
  PATH = "./out/"+DATASET+"/"
  files = list_files(PATH)

  gf_df = full_gf_df[full_gf_df["data"] == DATASET]
  gf_df = gf_df[['pattern', 'vertex_count', 'edge_count', 'diameter', 'density']]

  append_df = []
  for f in files:
    try:
      temp_df = pd.read_csv(PATH+f, header=None)
      temp_df["pattern"] = f.split(".")[0]
      append_df.append(temp_df)
    except EmptyDataError as e:
      continue

  print("Read:", DATASET)
  append_df = pd.concat(append_df, ignore_index=True)
  append_df.columns = ['vertex_id'] + orbits + ['pattern']
  append_df = append_df.loc[:, (append_df != 0).any(axis=0)]

  f_df = append_df.groupby(['pattern']).sum().reset_index().merge(gf_df, how="inner").fillna(0).drop(['pattern'], axis=1).groupby([to_group]).mean().reset_index().drop(to_drop, axis=1)
  melted_f_df = pd.melt(f_df, id_vars=[to_group])
  melted_f_df.columns = ["Vertex Count", "Orbit", "Count"]
  for product in cross_product:
    combination_exists = (melted_f_df['Vertex Count'] == product[0]) & (melted_f_df['Orbit'] == product[1])
    if not combination_exists.any():
        melted_f_df = pd.concat([melted_f_df, pd.DataFrame([{'Vertex Count': product[0], 'Orbit': product[1], 'Count': np.nan}])], ignore_index=True)

  melted_f_df = melted_f_df[~melted_f_df['Orbit'].isin(to_exclude)]

  print("Drawing:", DATASET)
  ax = fig.add_subplot(4, 2, i)
  sns.lineplot(data=melted_f_df, x="Orbit", y="Count", hue="Vertex Count", palette='tab10', ax=ax, marker="X")
  plt.xlabel(None)
  plt.ylabel(None)
  plt.title(DATASET.upper(), fontsize=9, fontweight='bold')
  plt.xticks(rotation=90)
  if i!=2:
    plt.legend([],[], frameon=False)
  else:
    sns.move_legend(ax, "upper left", bbox_to_anchor=(1, 1.06))
  i+=1

fig.text(0.5, 0.05, 'Orbits', ha='center', weight='bold')
fig.text(0.065, 0.5, 'Count', va='center', rotation='vertical', weight='bold')
plt.savefig('foo.pdf', bbox_inches='tight')

fig, ax = plt.subplots(figsize=(8,8))
for_corr = append_df.drop(["vertex_id", "pattern"], axis=1)
for_corr = for_corr.loc[:, (for_corr != 0).any(axis=0)]
sns.heatmap(for_corr.corr(method='pearson'), annot=True, cmap="coolwarm", fmt='.2f')
plt.title(DATASET)

vcount_df = full_gf_df.groupby(["data","vertex_count"]).count().reset_index()[["data","vertex_count","pattern"]]
vcount_df.columns = ["Data", "Vertex Count", "Count"]
vcount_df = vcount_df[vcount_df["Data"] != "test"]
fig, ax = plt.subplots(figsize=(8, 3))
sns.barplot(data=vcount_df, hue="Vertex Count", y="Count", x="Data", palette='tab10', ax=ax)
sns.move_legend(ax, "upper left", bbox_to_anchor=(1, 1.06))

f_dfs = []
for DATASET in DATASETS:
  append_df = []
  PATH = "./out/"+DATASET+"/"
  files = list_files(PATH)

  for f in files:
    try:
      temp_df = pd.read_csv(PATH+f, header=None)
      temp_df["pattern"] = f.split(".")[0]
      append_df.append(temp_df)
    except EmptyDataError as e:
      continue

  append_df = pd.concat(append_df, ignore_index=True)
  append_df.columns = ['vertex_id'] + orbits + ['pattern']

  gf_df = full_gf_df[full_gf_df["data"] == DATASET]
  gf_df = gf_df[['pattern', 'vertex_count', 'edge_count', 'diameter', 'density']]
  f_df = append_df.groupby(['pattern']).sum().reset_index().merge(gf_df, how="inner").fillna(0).drop(to_drop+orbits, axis=1)
  f_df["data"] = DATASET
  f_dfs.append(f_df)

f_dfs = pd.concat(f_dfs, ignore_index=True)
f_dfs = f_dfs.groupby(["data", "vertex_count"]).count().reset_index()
f_dfs

vcount_df

